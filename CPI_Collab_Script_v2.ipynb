{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuvaajithansightspectrum/CPI_Automation/blob/main/CPI_Collab_Script_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggvtbESOgWJ5",
        "outputId": "4d2a7bf4-181e-4e6e-bcc2-7fb23a139a4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/CPI_Automation/CPI_Input_Files/Req_Files/requirements.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/CPI_Automation/CPI_Input_Files/Req_Files/requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/CPI_Automation/CPI_Input_Files/Req_Files/requirements.txt (line 3)) (0.14.4)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/CPI_Automation/CPI_Input_Files/Req_Files/requirements.txt (line 4)) (3.1.5)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement shutil (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for shutil\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting rpy2==3.5.16\n",
            "  Downloading rpy2-3.5.16.tar.gz (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi>=1.15.1 in /usr/local/lib/python3.10/dist-packages (from rpy2==3.5.16) (1.17.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from rpy2==3.5.16) (3.1.4)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from rpy2==3.5.16) (5.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.15.1->rpy2==3.5.16) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->rpy2==3.5.16) (3.0.2)\n",
            "Building wheels for collected packages: rpy2\n",
            "  Building wheel for rpy2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rpy2: filename=rpy2-3.5.16-cp310-cp310-linux_x86_64.whl size=329847 sha256=6f12f4d85de68dabc0e686210bb1050f861d3d8280327588c372c11fc6e6bd6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/9b/1a/c09b2935ab01605117b270e2abb744a10f5b213c6f9b7213ce\n",
            "Successfully built rpy2\n",
            "Installing collected packages: rpy2\n",
            "  Attempting uninstall: rpy2\n",
            "    Found existing installation: rpy2 3.4.2\n",
            "    Uninstalling rpy2-3.4.2:\n",
            "      Successfully uninstalled rpy2-3.4.2\n",
            "Successfully installed rpy2-3.5.16\n",
            "Readed\n",
            "                                          LOGS        TIME\n",
            "0                    COICOP File:169 Readed at  2024-11-21\n",
            "1  Emirates Weightages File:169 Rows Readed at  2024-11-21\n",
            "2                  FUJ File:169 Rows Readed at  2024-11-21\n",
            "3                  RAK File:169 Rows Readed at  2024-11-21\n",
            "4                  ABD File:169 Rows Readed at  2024-11-21\n",
            "5                  DXB File:169 Rows Readed at  2024-11-21\n",
            "Estimation Process started...\n",
            "\n",
            " Abudhabi data updated to sheet AD in the Excel file.\n",
            "\n",
            "\n",
            " Dubai data updated to sheet DU in the Excel file.\n",
            "\n",
            "\n",
            " Abudhabi data updated to sheet AD in the Excel file.\n",
            "\n",
            "\n",
            " Dubai data updated to sheet DU in the Excel file.\n",
            "\n",
            "\n",
            " Abudhabi data updated to sheet AD in the Excel file.\n",
            "\n",
            "\n",
            " Dubai data updated to sheet DU in the Excel file.\n",
            "\n",
            "Abudhabi, Dubai data loaded to Estimation input files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: trying URL 'https://cran.rstudio.com/src/contrib/openxlsx_4.2.7.1.tar.gz'\n",
            "\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: Content type 'application/x-gzip'\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]:  length 1389194 bytes (1.3 MB)\n",
            "\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: =\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: \n",
            "\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: downloaded 1.3 MB\n",
            "\n",
            "\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: \n",
            "\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: \n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: The downloaded source packages are in\n",
            "\t‘/tmp/RtmptGPFgB/downloaded_packages’\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: \n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: \n",
            "\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: \n",
            "Attaching package: ‘dplyr’\n",
            "\n",
            "\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: The following objects are masked from ‘package:stats’:\n",
            "\n",
            "    filter, lag\n",
            "\n",
            "\n",
            "WARNING:rpy2.rinterface_lib.callbacks:R[write to console]: The following objects are masked from ‘package:base’:\n",
            "\n",
            "    intersect, setdiff, setequal, union\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Not enough test data for ISIC code: isic0512 after removing NAs\n",
            "\n",
            "Predictions saved to 'Predictions' sheet in the Excel file.\n",
            "Predictions Updated to UQ sheet in the Excel file.\n",
            "UQ Estimation Completed\n",
            "\n",
            "Not enough test data for ISIC code: isic0512 after removing NAs\n",
            "\n",
            "Predictions saved to 'Predictions' sheet in the Excel file.\n",
            "Predictions Updated to AJ sheet in the Excel file.\n",
            "AJ Estimation Completed\n",
            "\n",
            "Not enough test data for ISIC code: isic0512 after removing NAs\n",
            "\n",
            "Predictions saved to 'Predictions' sheet in the Excel file.\n",
            "Predictions Updated to SH sheet in the Excel file.\n",
            "SH Estimation Completed\n",
            "     CODE                           DESC_AR                        DESC_EN  \\\n",
            "0      00                     الإنفاق العام             Public Expenditure   \n",
            "1      01  الاغذية والمشروبات غير الكحولية   Food And Non-Alcoholic Drinks   \n",
            "2     011                           الاغذية                          Foods   \n",
            "3    0111                  الحبوب ومنتجاتها     Cereals And Their Products   \n",
            "4    0112                        اللحـــوم                            Meat   \n",
            "..    ...                               ...                            ...   \n",
            "164  1329           الامتعة الشخصية الأخرى          Other Personal Effects   \n",
            "165   133                الحماية الاجتماعية              Social Protection   \n",
            "166  1330                الحماية الاجتماعية              Social Protection   \n",
            "167   139                   الخدمات الاخرى                  Other Services   \n",
            "168  1390                   الخدمات الاخرى                  Other Services   \n",
            "\n",
            "     JAN_25_FUJ  JAN_25_RAK  JAN_25_DUB  JAN_25_ABU   JAN_25_UQ   JAN_25_AJ  \\\n",
            "0    106.310256  106.402826  111.563715  106.263444  105.753769  104.233638   \n",
            "1    112.776886  116.703722  114.247695  112.925491  112.228869  111.999476   \n",
            "2    114.574093  118.473681  114.396105  114.106683  113.369417  113.296872   \n",
            "3    115.064255  116.085822  117.955881  116.170978  116.677452  117.341589   \n",
            "4    108.089375  118.663794  112.705382  107.746943  108.611947  108.580523   \n",
            "..          ...         ...         ...         ...         ...         ...   \n",
            "164  115.284835  128.747209  120.970653  101.955772  119.612406  120.433198   \n",
            "165  101.476500   99.743177  101.476500  104.351678  101.476500  101.454075   \n",
            "166  101.476500   99.743177  101.476500  104.351678  101.476500  101.454075   \n",
            "167   97.817176  105.914949  102.182069  109.692670   96.757176  100.686803   \n",
            "168   97.817176  105.914949  102.182069  109.692670   96.757176  100.686803   \n",
            "\n",
            "      JAN_25_SH  \n",
            "0    106.896644  \n",
            "1    113.600889  \n",
            "2    114.507451  \n",
            "3    116.898858  \n",
            "4    110.558603  \n",
            "..          ...  \n",
            "164  122.617216  \n",
            "165  101.454075  \n",
            "166  101.454075  \n",
            "167  100.149680  \n",
            "168  100.149680  \n",
            "\n",
            "[169 rows x 10 columns]\n",
            "CPI Calculation Started..\n",
            "     CODE  JAN_25_FUJ   JAN_25_AJ  JAN_25_RAK   JAN_25_UQ  JAN_25_ABU  \\\n",
            "0      00  106.310256  104.233638  106.402826  105.753769  106.263444   \n",
            "1      01  112.776886  111.999476  116.703722  112.228869  112.925491   \n",
            "2     011  114.574093  113.296872  118.473681  113.369417  114.106683   \n",
            "3    0111  115.064255  117.341589  116.085822  116.677452  116.170978   \n",
            "4    0112  108.089375  108.580523  118.663794  108.611947  107.746943   \n",
            "..    ...         ...         ...         ...         ...         ...   \n",
            "164  1329  115.284835  120.433198  128.747209  119.612406  101.955772   \n",
            "165   133  101.476500  101.454075   99.743177  101.476500  104.351678   \n",
            "166  1330  101.476500  101.454075   99.743177  101.476500  104.351678   \n",
            "167   139   97.817176  100.686803  105.914949   96.757176  109.692670   \n",
            "168  1390   97.817176  100.686803  105.914949   96.757176  109.692670   \n",
            "\n",
            "     JAN_25_DUB   JAN_25_SH  JAN_25_UAE  \n",
            "0    111.563715  106.896644  108.163555  \n",
            "1    114.247695  113.600889  113.594420  \n",
            "2    114.396105  114.507451  114.409213  \n",
            "3    117.955881  116.898858  116.895133  \n",
            "4    112.705382  110.558603  110.109576  \n",
            "..          ...         ...         ...  \n",
            "164  120.970653  122.617216  122.670525  \n",
            "165  101.476500  101.454075  101.386636  \n",
            "166  101.476500  101.454075  101.386636  \n",
            "167  102.182069  100.149680  101.475631  \n",
            "168  102.182069  100.149680  101.475631  \n",
            "\n",
            "[169 rows x 9 columns]\n",
            "CPI Calculations Completed................\n",
            "Started wrting to ARCH files\n",
            "ARCH Data updated successfully\n",
            "File copied to /content/sample_data/Copy_Of_ARCH_TimeSeries.xlsx\n",
            "----------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Connecting To Drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Check if Google Drive is already mounted\n",
        "if not hasattr(drive, 'mounted') or not drive.mount('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print(\"Google Drive is already mounted.\")\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "#----------PIP Install Req File-----------------\n",
        "!pip install -r '/content/drive/MyDrive/CPI_Automation/CPI_Input_Files/Req_Files/requirements.txt'\n",
        "!pip install rpy2==3.5.16\n",
        "\n",
        "#----------------------------------------\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "#import shutil\n",
        "from datetime import datetime\n",
        "#Est Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import math\n",
        "from openpyxl import load_workbook\n",
        "import warnings\n",
        "#Imports to run R within Python\n",
        "import pandas as pd\n",
        "import rpy2.robjects as robjects\n",
        "from rpy2.robjects import pandas2ri\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# OUTPUT File Path\n",
        "Output_Path=r'/content/drive/MyDrive/CPI_Automation/CPI_Input_Files/OutPut_File/ARCH_TimeSeries1.xlsx'\n",
        "#Output_Path1=r'C:\\Users\\SSMEPZ\\3D Objects\\CP\\CPI\\2023\\test.xlsx'\n",
        "#LOG File Path\n",
        "Log_DF = pd.DataFrame(columns=['LOGS','TIME'])\n",
        "current_date = datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "Log_path=r'/content/drive/MyDrive/CPI_Automation/CPI_Input_Files/LOG\\log_'+current_date+'.txt'\n",
        "\n",
        "#=======Estimation File Path ============================================================\n",
        "\n",
        "UMQ_path =  r\"/content/drive/MyDrive/CPI_Automation/CPI_Input_Files/Estimation_Input_Files/bdd_UQ.xlsx\"\n",
        "UMQ_Sheet_Name=\"UQ\"\n",
        "\n",
        "AJ_path = r\"/content/drive/MyDrive/CPI_Automation/CPI_Input_Files/Estimation_Input_Files/bdd_AJ.xlsx\"\n",
        "AJ_Sheet_Name=\"AJ\"\n",
        "\n",
        "SH_path =  r\"/content/drive/MyDrive/CPI_Automation/CPI_Input_Files/Estimation_Input_Files/bdd_SH.xlsx\"\n",
        "SH_Sheet_Name=\"SH\"\n",
        "\n",
        "#======================Default file path to store predictions temp ===============================\n",
        "Temp_pred_path = r\"/content/drive/MyDrive/CPI_Automation/CPI_Input_Files/Estimation_Input_Files/Temp_Pred_File.xlsx\"\n",
        "\n",
        "\n",
        "#============================ Input File Path ===========================================\n",
        "# Emirates codes for input FUJ, RAK, DXB, ABD.\n",
        "\n",
        "# =============== Input processing MONTH (3 digit) & YEAR(2 didgit)  Ex: Month: JAN , Year=24.(If you are going to process January 2024 data)\n",
        "param_path = r'/content/sample_data/PARAM.xlsx'\n",
        "\n",
        "dtype_dict = {\n",
        "    'CODE_INDX': 'str',\n",
        "    'CPI_INDX': 'str',\n",
        "    'START_ROW': 'int64',\n",
        "    'END_ROW': 'int64',\n",
        "    'MONTH': 'str',\n",
        "    'YEAR': 'str'\n",
        "}\n",
        "try:\n",
        "    param_df = pd.read_excel(param_path,dtype=dtype_dict) #dtype=dtype_dict\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at path '{param_path}'\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "def extract_row_variables(df, emirate, col):\n",
        "    retuen_val = df.loc[df['EMIRATE'] == emirate, col].values\n",
        "    if len(retuen_val) == 0:\n",
        "        return []\n",
        "    value = retuen_val[0]\n",
        "\n",
        "    if isinstance(value, str) and ',' in value or col=='CODE_INDX' or col=='CPI_INDX':\n",
        "        return [int(x.strip()) for x in value.split(',')]\n",
        "    elif col=='START_ROW' or col=='END_ROW':\n",
        "        return int(value)\n",
        "    else:\n",
        "        return str(value)\n",
        "\n",
        "\n",
        "prc_month=extract_row_variables(param_df,'FUJ','MONTH')\n",
        "prc_year=extract_row_variables(param_df,'FUJ','YEAR')\n",
        "\n",
        "\n",
        "#=========================================================================================\n",
        "\n",
        "\n",
        "#--FUJ\n",
        "FUJ_path = r'/content/sample_data/FUJ.xlsx'    # Path of the input file\n",
        "FUJ_code_col_to_read = extract_row_variables(param_df,'FUJ','CODE_INDX')  # Ex: if CODE exists in A cell means its index is '0', if 'B' means 1 and so on. (Ex value '0' if CODE in A cell)\n",
        "FUJ_cpi_col_to_read = extract_row_variables(param_df,'FUJ','CPI_INDX') # Index of the Excel cell where CPI values exist Ex: if CPI value in C column of Excel means the value will be '2'.\n",
        "FUJ_start_row=extract_row_variables(param_df,'FUJ','START_ROW') # Row number appears in Excel where CPI first value starts (Ex: if CPI value of CODE 00 start at row number 7 menas we have to give it as input)\n",
        "FUJ_end_row=extract_row_variables(param_df,'FUJ','END_ROW') #  Row number appears in Excel where CPI last value ends (Ex: if last CPI is at row number 174th row menas we have to give it as input 174)\n",
        "FUJ_cpi_column_name = [prc_month+'_'+prc_year+'_FUJ']\n",
        "\n",
        "#--RAK\n",
        "RAK_path = r'/content/sample_data/RAK.xlsx'\n",
        "RAK_code_col_to_read = extract_row_variables(param_df,'RAK','CODE_INDX') # Ex: if CODE exists in A cell means its index is '0', if 'B' means 1 and so on. (Ex value '0' if CODE in A cell)\n",
        "RAK_cpi_col_to_read = extract_row_variables(param_df,'RAK','CPI_INDX') # Index of the Excel cell where CPI values exist Ex: if CPI value in C column of Excel means the value will be '2'.\n",
        "RAK_start_row=extract_row_variables(param_df,'RAK','START_ROW') # Row number appears in Excel where CPI first value starts (Ex: if CPI value of CODE 00 start at row number 7 menas we have to give it as input)\n",
        "RAK_end_row=extract_row_variables(param_df,'RAK','END_ROW') #  Row number appears in Excel where CPI last value ends (Ex: if last CPI is at row number 174th row menas we have to give it as input 174)\n",
        "RAK_cpi_column_name = [prc_month+'_'+prc_year+'_RAK']\n",
        "\n",
        "\n",
        "#--ABD\n",
        "ABD_path = r'/content/sample_data/ABD.xlsx'\n",
        "ABD_code_col_to_read = extract_row_variables(param_df,'ABD','CODE_INDX') # Ex: if CODE exists in A cell means its index is '0', if 'B' means 1 and so on. (Ex value '0' if CODE in A cell)\n",
        "ABD_cpi_col_to_read = extract_row_variables(param_df,'ABD','CPI_INDX') # Index of the Excel cell where CPI values exist Ex: if CPI value in C column of Excel means the value will be '2'.\n",
        "ABD_start_row=extract_row_variables(param_df,'ABD','START_ROW') # Row number appears in Excel where CPI first value starts (Ex: if CPI value of CODE 00 start at row number 7 menas we have to give it as input)\n",
        "ABD_end_row=extract_row_variables(param_df,'ABD','END_ROW') #  Row number appears in Excel where CPI last value ends (Ex: if last CPI is at row number 174th row menas we have to give it as input 174)\n",
        "ABD_cpi_column_name = [prc_month+'_'+prc_year+'_ABU']\n",
        "\n",
        "#--DXB\n",
        "DXB_path = r'/content/sample_data/DUB.xlsx'\n",
        "DXB_code_col_to_read = extract_row_variables(param_df,'DUB','CODE_INDX') # Ex: if CODE exists in combined cells like A,B and C means. We need to give its all index value as input.\n",
        "DXB_cpi_col_to_read =extract_row_variables(param_df,'DUB','CPI_INDX') # Index of the Excel cell where CPI values exist Ex: if CPI value in C column of Excel means the value will be '2'.\n",
        "DXB_start_row=extract_row_variables(param_df,'DUB','START_ROW')   # Row number appears in Excel where CPI first value starts (Ex: if CPI value of CODE 00 start at row number 7 menas we have to give it as input)\n",
        "DXB_end_row=extract_row_variables(param_df,'DUB','END_ROW') #  Row number appears in Excel where CPI last value ends (Ex: if last CPI is at row number 174th row menas we have to give it as input 174)\n",
        "DXB_cpi_column_name = [prc_month+'_'+prc_year+'_DUB']\n",
        "\n",
        "\n",
        "#========================= Flags Def ===========================\n",
        "\n",
        "coicop_flg=0\n",
        "emi_weight_flg=0\n",
        "fuj_rd_flg=0\n",
        "rak_rd_flg=0\n",
        "abd_rd_flg=0\n",
        "dub_rd_flg=0\n",
        "estimation_flg=0\n",
        "arch_flg=0\n",
        "\n",
        "print('Readed')\n",
        "#==============================COICOP FILE READ START (Default File Path)==================================================================\n",
        "COICOP_path = r'/content/drive/MyDrive/CPI_Automation/CPI_Input_Files/Std_COICOP_Code/COICOP_Lvls.xlsx'\n",
        "columns_to_read = [0,1,2]\n",
        "new_column_names = ['CODE','DESC_AR','DESC_EN']\n",
        "start_row=0\n",
        "end_row=170\n",
        "try:\n",
        "    if not os.path.exists(COICOP_path):\n",
        "        raise FileNotFoundError(f\"The file '{COICOP_path}' does not exist.\")\n",
        "    else:\n",
        "        COICOP_DF= pd.read_excel(COICOP_path, usecols=columns_to_read, skiprows=start_row, nrows=(end_row - start_row),dtype=str, engine='openpyxl' )\n",
        "        COICOP_DF.columns = new_column_names\n",
        "        for col in new_column_names:\n",
        "            COICOP_DF[col] = COICOP_DF[col].astype(str)\n",
        "        #print(COICOP_DF.dtypes)\n",
        "        #print(COICOP_DF,'COIP')\n",
        "        COICOP_DF_rows = COICOP_DF.shape[0]\n",
        "        Log_DF.loc[len(Log_DF)] = ['COICOP File:'+str(COICOP_DF_rows)+' Readed at', current_date]\n",
        "        coicop_flg=1\n",
        "except FileNotFoundError as e:\n",
        "    Log_DF.loc[len(Log_DF)] = [e, current_date]\n",
        "    coicop_flg=0\n",
        "#==================================== Weightage File Read (Default File Path) =============================\n",
        "\n",
        "#for 4 Emirates Providing data\n",
        "Weightage_path = r'/content/drive/MyDrive/CPI_Automation/CPI_Input_Files/Std_CPI_weightage/CPI_WEIGHTAGES.xlsx'\n",
        "\n",
        "#For 3 Emirates which we will calc CPI using Estimation\n",
        "Weight_path=r'/content/drive/MyDrive/CPI_Automation/CPI_Input_Files/Std_EST_Weightage/Est_Weight_Uq_Aj_Sh.xlsx'\n",
        "\n",
        "try:\n",
        "    if not os.path.exists(Weightage_path):\n",
        "        raise FileNotFoundError(f\"The file '{FUJ_path}' does not exist.\")\n",
        "    else:\n",
        "        Emirate_Weightage = pd.read_excel(Weightage_path, dtype={'CODE':str}, engine='openpyxl')\n",
        "        #columns_to_convert = ['ABU_DHABI','DUBAI','SHARJAH','AJMAN','UMM_AL_QUWAIN','RAS_AL_KHAIMAH','FUJAIRAH','STATE_WEIGHT_BY_STATE_TO_HOUSEHOLDS','COUNTRY_WEIGHT_BY_COUNTRY_FOR_EMIRATI_FAMILI']\n",
        "        #columns_to_convert = ['ABU_WEIGHT','DUB_WEIGHT','SHJ_WEIGHT','AJM_WEIGHT','UQM_WEIGHT','RAK_WEIGHT','FUJ_WEIGHT','GLOBALE','ALL_EMI_MUL_WEIGHT']\n",
        "        #for column in columns_to_convert:\n",
        "            #Emirate_Weightage[column] = pd.to_numeric(Emirate_Weightage[column], errors='coerce')\n",
        "        Emirate_Weightage_rows = Emirate_Weightage.shape[0]\n",
        "        Log_DF.loc[len(Log_DF)] = ['Emirates Weightages File:'+str(Emirate_Weightage_rows)+' Rows Readed at', current_date]\n",
        "        emi_weight_flg=1\n",
        "except FileNotFoundError as e:\n",
        "    Log_DF.loc[len(Log_DF)] = [e, current_date]\n",
        "    emi_weight_flg=0\n",
        "#===============================================================================================================\n",
        "#==================================  READ INPUT FILES ===================================================\n",
        "\n",
        "\n",
        "def Read_Excel(path,col_index,cpi_index,start_row,end_row,cpi_column_name,datatype):\n",
        "    read_Excel_df = pd.read_excel(path,dtype=datatype, skiprows=start_row-2, nrows=(end_row - start_row)+1,engine='openpyxl' ) #dtype=str\n",
        "    #print(read_Excel_df)\n",
        "    #with pd.ExcelWriter(path, engine='openpyxl', mode='a') as writer:\n",
        "        #read_Excel_df.to_excel(writer, sheet_name='NewSheetName3', index=False)\n",
        "    concat_code_col = read_Excel_df.iloc[:, col_index]\n",
        "    concat_cpi_col = read_Excel_df.iloc[:, cpi_index]\n",
        "    concat_code_col = concat_code_col.fillna('')\n",
        "    read_Excel_df['CODE'] = concat_code_col.astype(str).agg(''.join, axis=1)\n",
        "    read_Excel_df[cpi_column_name] = concat_cpi_col\n",
        "    v_read_Excel_df=read_Excel_df[['CODE']+cpi_column_name]\n",
        "    v_read_Excel_df.at[0, 'CODE'] = '00'\n",
        "    v_read_Excel_df = v_read_Excel_df.assign(CODE=v_read_Excel_df['CODE'].str.replace(r'[_\\*\\.\\-\\ ]', '', regex=True))\n",
        "    v_read_Excel_df[v_read_Excel_df['CODE'].str.len() <= 4]\n",
        "    def def_level(codes):\n",
        "        prefix = \"\"\n",
        "        result = []\n",
        "        for code in codes:\n",
        "            if len(code) == 2 and code in ['01','02','03','04','05','06','07','08','09','10','11','12','13']:\n",
        "                prefix = f\"{code}\"\n",
        "                result.append(code)\n",
        "            else:\n",
        "                result.append(f\"{prefix}\")\n",
        "        return result\n",
        "    def add_zero(code, other_value):\n",
        "        if other_value in ['01','02','03','04','05','06','07','08','09'] and not code.startswith('0'):\n",
        "            return f\"0{code}\"\n",
        "        return code\n",
        "\n",
        "    v_read_Excel_df['CODE_LVL'] = def_level(v_read_Excel_df['CODE'])\n",
        "    v_read_Excel_df['CODE'] = v_read_Excel_df.apply(lambda row: add_zero(row['CODE'], row['CODE_LVL']), axis=1)\n",
        "    v_read_Excel_df['CODE']=v_read_Excel_df['CODE'].astype(str)\n",
        "    #print(v_read_Excel_df)\n",
        "    #with pd.ExcelWriter(path, engine='openpyxl', mode='a') as writer:\n",
        "        #v_read_Excel_df.to_excel(writer, sheet_name='NewSheetName11', index=False)\n",
        "    Merged_DF = pd.merge(COICOP_DF['CODE'],v_read_Excel_df, on='CODE', how='left')\n",
        "    return Merged_DF[['CODE']+cpi_column_name]\n",
        "\n",
        "\n",
        "#FUJ\n",
        "try:\n",
        "    if not os.path.exists(FUJ_path):\n",
        "        raise FileNotFoundError(f\"The file '{FUJ_path}' does not exist.\")\n",
        "    else:\n",
        "        fuj_d_type = {key: str for key in FUJ_code_col_to_read}\n",
        "        #print(fuj_d_type)\n",
        "        FUJ_Merged=Read_Excel(FUJ_path,FUJ_code_col_to_read,FUJ_cpi_col_to_read,FUJ_start_row,FUJ_end_row,FUJ_cpi_column_name,fuj_d_type)\n",
        "        FUJ_Merged_rows = FUJ_Merged.shape[0]\n",
        "        Log_DF.loc[len(Log_DF)] = ['FUJ File:'+str(FUJ_Merged_rows)+' Rows Readed at', current_date]\n",
        "        fuj_rd_flg=1\n",
        "except FileNotFoundError as e:\n",
        "    FUJ_Merged=pd.DataFrame()\n",
        "    Log_DF.loc[len(Log_DF)] = [e, current_date]\n",
        "    fuj_rd_flg=0\n",
        "\n",
        "#RAK\n",
        "try:\n",
        "    if not os.path.exists(RAK_path):\n",
        "        raise FileNotFoundError(f\"The file '{RAK_path}' does not exist.\")\n",
        "    else:\n",
        "        rak_d_type = {key: str for key in RAK_code_col_to_read}\n",
        "        #print(rak_d_type)\n",
        "        RAK_Merged=Read_Excel(RAK_path,RAK_code_col_to_read,RAK_cpi_col_to_read,RAK_start_row,RAK_end_row,RAK_cpi_column_name,rak_d_type)\n",
        "        RAK_Merged_rows = RAK_Merged.shape[0]\n",
        "        Log_DF.loc[len(Log_DF)] = ['RAK File:'+str(RAK_Merged_rows)+' Rows Readed at', current_date]\n",
        "        rak_rd_flg=1\n",
        "except FileNotFoundError as e:\n",
        "    RAK_Merged=pd.DataFrame()\n",
        "    Log_DF.loc[len(Log_DF)] = [e, current_date]\n",
        "    fuj_rd_flg=0\n",
        "\n",
        "#ABD\n",
        "try:\n",
        "    if not os.path.exists(ABD_path):\n",
        "        raise FileNotFoundError(f\"The file '{ABD_path}' does not exist.\")\n",
        "    else:\n",
        "        abd_d_type = {key: str for key in ABD_code_col_to_read}\n",
        "        #print(abd_d_type)\n",
        "        ABD_Merged=Read_Excel(ABD_path,ABD_code_col_to_read,ABD_cpi_col_to_read,ABD_start_row,ABD_end_row,ABD_cpi_column_name,abd_d_type)\n",
        "        ABD_Merged_rows = ABD_Merged.shape[0]\n",
        "        Log_DF.loc[len(Log_DF)] = ['ABD File:'+str(ABD_Merged_rows)+' Rows Readed at', current_date]\n",
        "        abd_rd_flg=1\n",
        "except FileNotFoundError as e:\n",
        "    ABD_Merged=pd.DataFrame()\n",
        "    Log_DF.loc[len(Log_DF)] = [e, current_date]\n",
        "    abd_rd_flg=0\n",
        "\n",
        "#DXB\n",
        "try:\n",
        "    if not os.path.exists(DXB_path):\n",
        "        raise FileNotFoundError(f\"The file '{DXB_path}' does not exist.\")\n",
        "    else:\n",
        "        dxb_d_type = {key: str for key in DXB_code_col_to_read}\n",
        "        #print(dxb_d_type)\n",
        "        DXB_Merged=Read_Excel(DXB_path,DXB_code_col_to_read,DXB_cpi_col_to_read,DXB_start_row,DXB_end_row,DXB_cpi_column_name,dxb_d_type)\n",
        "        DXB_Merged_rows = DXB_Merged.shape[0]\n",
        "        Log_DF.loc[len(Log_DF)] = ['DXB File:'+str(DXB_Merged_rows)+' Rows Readed at', current_date]\n",
        "        dub_rd_flg=1\n",
        "        #print(DXB_Merged)\n",
        "except FileNotFoundError as e:\n",
        "    DXB_Merged=pd.DataFrame()\n",
        "    Log_DF.loc[len(Log_DF)] = [e, current_date]\n",
        "    dub_rd_flg\n",
        "\n",
        "\n",
        "print(Log_DF)\n",
        "#print(Log_DF.iloc[[-1]])\n",
        "#print(FUJ_Merged)\n",
        "#print(RAK_Merged)\n",
        "#print(DXB_Merged)\n",
        "#print(ABD_Merged)\n",
        "\n",
        "#FUJ_Merged_DF = pd.merge(COICOP_DF['CODE'],FUJ_Merged, on='CODE', how='left')\n",
        "#RAK_Merged_DF = pd.merge(FUJ_Merged_DF,RAK_Merged, on='CODE', how='left')\n",
        "#DXB_Merged_DF = pd.merge(RAK_Merged_DF,DXB_Merged, on='CODE', how='left')\n",
        "#ABD_Merged_DF = pd.merge(DXB_Merged_DF,ABD_Merged, on='CODE', how='left')\n",
        "\n",
        "list_DF=[FUJ_Merged,RAK_Merged,DXB_Merged,ABD_Merged]\n",
        "Consoldtd_DF=COICOP_DF\n",
        "for df in list_DF:\n",
        "    if not df.empty:\n",
        "        try:\n",
        "            Consoldtd_DF = Consoldtd_DF.merge(df, on='CODE', how='left')\n",
        "        except Exception as e:\n",
        "            Log_DF.loc[len(Log_DF)] = ['Error while concat DF readed files: '+str(e), current_date]\n",
        "            print('Error while concat DF readed files: '+str(e))\n",
        "#print(Consoldtd_DF)\n",
        "\n",
        "#===============================================Estimation =============================================\n",
        "#v_UQ_AJ_SH_CPI_VAL=pd.DataFrame()\n",
        "def Estimation_Phase(EST_FLG):\n",
        "    if EST_FLG==1:\n",
        "        global estimation_flg\n",
        "        print(\"Estimation Process started...\")\n",
        "        try:\n",
        "            #======================================================= Flaten Dubai and AbuDhabi Data ============================================\n",
        "            def convrt_str_date(date_str):\n",
        "                try:\n",
        "                    month = date_str[:3]\n",
        "                    year = date_str[4:6]\n",
        "                    month_mapping = {\n",
        "                        'JAN': '01', 'FEB': '02', 'MAR': '03', 'APR': '04',\n",
        "                        'MAY': '05', 'JUN': '06', 'JUL': '07', 'AUG': '08',\n",
        "                        'SEP': '09', 'OCT': '10', 'NOV': '11', 'DEC': '12'}\n",
        "                    year = '20' + year\n",
        "                    #formatted_date = f\"01-{month_mapping.get(month, '01')}-{year}\"\n",
        "                    #formatted_date = f\"{year}-{month_mapping.get(month, '01')}-01\"\n",
        "                    date_str = f\"{year}-{month_mapping.get(month, '12')}-01\"\n",
        "                    formatted_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
        "                    formatted_date_str = formatted_date.strftime(\"%Y-%m-%d\")\n",
        "                    return formatted_date_str\n",
        "                except Exception as e:\n",
        "                    print('Error while converting date: '+str(e))\n",
        "                    Log_DF.loc[len(Log_DF)] = ['Error while converting date: '+str(e), current_date]\n",
        "            def flaten_DF(AD_DU_DF):\n",
        "                try:\n",
        "                    #print('YUI',AD_DU_DF)\n",
        "                    #print(AD_DU_DF.columns[1])\n",
        "                    filtered_df = AD_DU_DF[AD_DU_DF['CODE'].str.len() == 4]\n",
        "                    transposed_df = filtered_df.set_index('CODE').transpose()\n",
        "                    new_column = pd.DataFrame({'date': []})\n",
        "                    transposed_df.insert(0, 'date', [convrt_str_date(AD_DU_DF.columns[1])])\n",
        "                    return transposed_df\n",
        "                except Exception as e:\n",
        "                    print('Error while flatenning AD, DU df: '+str(e), current_date)\n",
        "                    Log_DF.loc[len(Log_DF)] = ['Error while flatenning AD, DU df: '+str(e), current_date]\n",
        "\n",
        "            Flat_DXB=flaten_DF(DXB_Merged)\n",
        "            #print(Flat_DXB)\n",
        "            Flat_DXB.columns = ['isic' + col if col.lower() != 'date' else col for col in Flat_DXB.columns]\n",
        "            Flat_ABD=flaten_DF(ABD_Merged)\n",
        "            Flat_ABD.columns = ['isic' + col if col.lower() != 'date' else col for col in Flat_ABD.columns]\n",
        "            #print(Flat_DXB)\n",
        "            #print(Flat_ABD)\n",
        "            #exit()  # or sys.exit()\n",
        "            #=======================================================================================\n",
        "\n",
        "            #print(Log_DF.iloc[[-1]])\n",
        "\n",
        "\n",
        "            def upd_dubai_data(Path):\n",
        "                #print('Upd Dubai')\n",
        "                try:\n",
        "                    UQ_Check_Du =pd.read_excel(Path, sheet_name='DU', engine='openpyxl') #dtype=str\n",
        "                    UQ_Check_Du['date'] = pd.to_datetime(UQ_Check_Du['date']).dt.strftime('%Y-%m-%d')\n",
        "                    #v_Chk_max_date=datetime.strptime(str(UQ_Check_Du.iloc[:,0].max())[0:10], '%Y-%m-%d')\n",
        "                    #UQ_Check_Du.iloc[:, 0] = pd.to_datetime(UQ_Check_Du.iloc[:, 0],format='%Y-%m-%d') # infer_datetime_format=True) #format='%d/%m/%Y %I:%M:%S %p')\n",
        "                    v_Chk_max_date = UQ_Check_Du.iloc[:, 0].max()\n",
        "                    #v_curr_date=datetime.strptime(str(Flat_DXB.iloc[:,0].max()), '%Y-%m-%d')\n",
        "                    #Flat_DXB.iloc[:, 0] = pd.to_datetime(Flat_DXB.iloc[:, 0],format='%Y-%m-%d') # infer_datetime_format=True) #format='%d/%m/%Y %I:%M:%S %p')\n",
        "                    v_curr_date = Flat_DXB.iloc[:, 0].max()\n",
        "                    #print(v_Chk_max_date)\n",
        "                    #print(v_curr_date)\n",
        "                    if v_Chk_max_date<v_curr_date:\n",
        "                        updated_data = pd.concat([UQ_Check_Du, Flat_DXB], ignore_index=True)\n",
        "                        #book = load_workbook(Path)\n",
        "                        #sheet_name = 'DU'\n",
        "                        #sheet = book[sheet_name]\n",
        "                        #last_row = sheet.max_row\n",
        "                        #for col_idx, value in enumerate(Flat_DXB.iloc[0], start=1):\n",
        "                            #sheet.cell(row=last_row + 1, column=col_idx, value=value)\n",
        "                        #book.save(Path)\n",
        "                        #book.close()\n",
        "                        with pd.ExcelWriter(Path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
        "                                updated_data.to_excel(writer, index=False, sheet_name='DU')\n",
        "                        print(\"\\n Dubai data updated to sheet DU in the Excel file.\\n\")\n",
        "                        Log_DF.loc[len(Log_DF)] = ['Dubai data updated to sheet DU in the Excel file.', current_date]\n",
        "                        return 1\n",
        "                    else:\n",
        "                        if v_Chk_max_date==v_curr_date:\n",
        "                            print(\"\\n Dubai data of date: \"+str(Flat_DXB.iloc[:,0].max())+\" Already  in sheet DU in the Excel file.\\n\")\n",
        "                            Log_DF.loc[len(Log_DF)] = ['Dubai data of date: '+str(Flat_DXB.iloc[:,0].max())+' Already  in sheet DU in the Excel file.', current_date]\n",
        "                            return 1\n",
        "                        else:\n",
        "                            return 0\n",
        "                except Exception as e:\n",
        "                    print(str(e))\n",
        "\n",
        "            def upd_Abu_data(Path):\n",
        "                #print('i')\n",
        "                try:\n",
        "                    UQ_Check_AD =pd.read_excel(Path, sheet_name='AD', engine='openpyxl') #dtype=str\n",
        "                    UQ_Check_AD['date'] = pd.to_datetime(UQ_Check_AD['date']).dt.strftime('%Y-%m-%d')\n",
        "                    #print('CHAK_DATE',UQ_Check_AD)\n",
        "                    #v_Chk_max_date=datetime.strptime(str(UQ_Check_AD.iloc[:,0].max())[0:10], '%Y-%m-%d')\n",
        "                    #UQ_Check_AD.iloc[:, 0] = pd.to_datetime(UQ_Check_AD.iloc[:, 0],format='%Y-%m-%d') # infer_datetime_format=True) #format='%d/%m/%Y %I:%M:%S %p')\n",
        "                    v_Chk_max_date = UQ_Check_AD.iloc[:, 0].max()\n",
        "                   # print(v_Chk_max_date)\n",
        "                    #v_curr_date=datetime.strptime(str(Flat_ABD.iloc[:,0].max()), '%Y-%m-%d')\n",
        "                    #Flat_ABD.iloc[:, 0] = pd.to_datetime(Flat_ABD.iloc[:, 0],format='%Y-%m-%d') # infer_datetime_format=True) #format='%d/%m/%Y %I:%M:%S %p')\n",
        "                    v_curr_date = Flat_ABD.iloc[:, 0].max()\n",
        "                    #print(v_Chk_max_date)\n",
        "                    #print(v_curr_date)\n",
        "                    if v_Chk_max_date<v_curr_date:\n",
        "                        updated_data = pd.concat([UQ_Check_AD, Flat_ABD], ignore_index=True)\n",
        "                        #print('AD',Flat_ABD)\n",
        "                        #print(Flat_ABD, pd.DataFrame)\n",
        "                        #book = load_workbook(Path)\n",
        "                        #sheet_name = 'AD'  # Change this to your actual sheet name\n",
        "                        #sheet = book[sheet_name]\n",
        "                        #last_row = sheet.max_row\n",
        "                        #for col_idx, value in enumerate(Flat_ABD.iloc[0], start=1):  # Use iloc[0] to get the first (and only) row\n",
        "                            #sheet.cell(row=last_row + 1, column=col_idx, value=value)\n",
        "                        #book.save(Path)\n",
        "                        #book.close()\n",
        "                        with pd.ExcelWriter(Path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
        "                                updated_data.to_excel(writer, index=False, sheet_name='AD')\n",
        "                        print(\"\\n Abudhabi data updated to sheet AD in the Excel file.\\n\")\n",
        "                        Log_DF.loc[len(Log_DF)] = ['Abudhabi data updated to sheet AD in the Excel file.', current_date]\n",
        "                        return 1\n",
        "                    else:\n",
        "                        if v_Chk_max_date==v_curr_date:\n",
        "                            print(\"\\n Abudhabi data of date: \"+str(Flat_ABD.iloc[:,0].max())+\" Already  in sheet AD in the Excel file.\\n\")\n",
        "                            Log_DF.loc[len(Log_DF)] = ['Abudhabi data of date: '+str(Flat_ABD.iloc[:,0].max())+' Already  in sheet AD in the Excel file.', current_date]\n",
        "                            return 1\n",
        "                        else:\n",
        "                            return 0\n",
        "                except Exception as e:\n",
        "                    print(str(e))\n",
        "\n",
        "\n",
        "            # ==================================== Start Estimations ============================================================\n",
        "\n",
        "\n",
        "            # Define the file path\n",
        "            #file_path = \"C:/Users/SSMEPZ/3D Objects/CP/CPI/MissingEmirate_Calc/TEST/bdd_UQ.xlsx\"\n",
        "            #Sheet_Name=\"UQ\"\n",
        "            def ESTIMATIONS(file_path,Sheet_Name,Temp_pred):\n",
        "                # Load the data\n",
        "                try:\n",
        "                    pandas2ri.deactivate()\n",
        "                    pandas2ri.activate()\n",
        "                    in_file_path=file_path.replace(\"\\\\\", \"/\")\n",
        "                    temp_path=Temp_pred\n",
        "                    Emirate = pd.read_excel(file_path, sheet_name=Sheet_Name)\n",
        "                    abd = pd.read_excel(file_path, sheet_name=\"AD\")\n",
        "                    ad= abd[:-1]\n",
        "                    #ad= abd\n",
        "                    dub = pd.read_excel(file_path, sheet_name=\"DU\")\n",
        "                    du= dub[:-1]\n",
        "                    #du= dub\n",
        "                    #ad_test = pd.read_excel(file_path, sheet_name=\"AD_test\")\n",
        "                    ad_test=abd.iloc[-2:]\n",
        "                    #du_test = pd.read_excel(file_path, sheet_name=\"DU_test\")\n",
        "                    du_test=dub.iloc[-2:]\n",
        "                    #print(Emirate)\n",
        "                    #print(ad)\n",
        "                    #print(du)\n",
        "                    #print(ad_test)\n",
        "                    #print(du_test)\n",
        "\n",
        "                    Emirate_r = pandas2ri.py2rpy(Emirate)\n",
        "                    ad_r = pandas2ri.py2rpy(ad)\n",
        "                    du_r = pandas2ri.py2rpy(du)\n",
        "                    ad_test_r = pandas2ri.py2rpy(ad_test)\n",
        "                    du_test_r = pandas2ri.py2rpy(du_test)\n",
        "                    #path_r = pandas2ri.py2rpy(in_file_path)\n",
        "                    robjects.globalenv['aj'] = Emirate_r\n",
        "                    robjects.globalenv['ad'] = ad_r\n",
        "                    robjects.globalenv['du'] = du_r\n",
        "                    robjects.globalenv['ad_test'] = ad_test_r\n",
        "                    robjects.globalenv['du_test'] = du_test_r\n",
        "\n",
        "                    #r.assign('aj', Emirate_r)\n",
        "                    #r.assign('ad', ad_r)\n",
        "                    #r.assign('du', du_r)\n",
        "                    #r.assign('ad_test', ad_test_r)\n",
        "                    #r.assign('du_test', du_test_r)\n",
        "                    robjects.globalenv['file_path'] = robjects.StrVector([in_file_path])\n",
        "                    robjects.globalenv['temp_pred_path'] = robjects.StrVector([temp_path])\n",
        "\n",
        "                    # To remove Predictions sheet\n",
        "\n",
        "\n",
        "                    r_code = \"\"\"\n",
        "                    # Load necessary libraries\n",
        "                    #install.packages(\"openxlsx\")\n",
        "                    #install.packages(\"dplyr\")\n",
        "                    if (!requireNamespace(\"dplyr\", quietly = TRUE)) install.packages(\"dplyr\")\n",
        "                    if (!requireNamespace(\"openxlsx\", quietly = TRUE)) install.packages(\"openxlsx\")\n",
        "                    library(dplyr)\n",
        "                    library(openxlsx)\n",
        "                    #print(aj)\n",
        "                    #print(ad)\n",
        "                    #print(du)\n",
        "                    #print(ad_test)\n",
        "                    #print(du_test)\n",
        "                    # Function to calculate log differences row-wise for each ISIC variable\n",
        "                    log_diff <- function(df) {\n",
        "                      df_diff <- df\n",
        "                      for (i in 2:nrow(df)) {\n",
        "                        for (j in 2:ncol(df)) {\n",
        "                          if (!is.na(df[i, j]) && !is.na(df[i-1, j]) && df[i-1, j] != 0) {\n",
        "                            df_diff[i, j] <- log(df[i, j] / df[i-1, j])\n",
        "                          } else {\n",
        "                            df_diff[i, j] <- NA\n",
        "                          }\n",
        "                        }\n",
        "                      }\n",
        "                      return(df_diff[-1, ])  # Remove the first row as it will have NAs\n",
        "                    }\n",
        "\n",
        "                    # Calculate log differences for AJ, DU, AD, AD_test, and DU_test\n",
        "                    aj_log_diff <- log_diff(aj)\n",
        "                    du_log_diff <- log_diff(du)\n",
        "                    ad_log_diff <- log_diff(ad)\n",
        "                    du_test_log_diff <- log_diff(du_test)\n",
        "                    ad_test_log_diff <- log_diff(ad_test)\n",
        "\n",
        "                    # Perform regression analysis for each ISIC code and make predictions\n",
        "                    isic_codes <- colnames(aj)[-1]  # Exclude the 'date' column\n",
        "\n",
        "                    predictions <- list()\n",
        "\n",
        "                    for (code in isic_codes) {\n",
        "                      # Prepare the data for log_diff_AJ ~ log_diff_DU + log_diff_AD\n",
        "                      data <- data.frame(\n",
        "                        AJ = aj_log_diff[[code]],\n",
        "                        DU = du_log_diff[[code]],\n",
        "                        AD = ad_log_diff[[code]]\n",
        "                      )\n",
        "\n",
        "                      # Remove rows with NA values\n",
        "                      data <- na.omit(data)\n",
        "\n",
        "                      # Check if data is empty after removing NAs\n",
        "                      if (nrow(data) > 0) {\n",
        "                        # Perform the regression\n",
        "                        model <- lm(AJ ~ DU + AD, data = data)\n",
        "                        model_D <- lm(AJ ~ DU, data = data)\n",
        "                        model_AD <- lm(AJ ~ AD, data = data)\n",
        "\n",
        "                        # Check significance of all coefficients in the full model\n",
        "                        summary_model <- summary(model)\n",
        "                        p_values <- summary_model$coefficients[, 4]\n",
        "\n",
        "                        #############################################################\n",
        "                        # Prepare the test data\n",
        "                        test_data <- data.frame(\n",
        "                          DU = du_test_log_diff[[code]],\n",
        "                          AD = ad_test_log_diff[[code]]\n",
        "                        )\n",
        "\n",
        "                        # Remove rows with NA values\n",
        "                        test_data <- na.omit(test_data)\n",
        "                        test_data_du <- test_data[ , \"DU\", drop = FALSE]\n",
        "                        test_data_ad <- test_data[ , \"AD\", drop = FALSE]\n",
        "\n",
        "                        # Make predictions based on model selection\n",
        "                        if (nrow(test_data) > 0) {\n",
        "                          if (!is.na(p_values[2] <= 0.05) && !is.na(p_values[3] <= 0.05)) {\n",
        "                            # Use the full model if any coefficient is significant\n",
        "                            pred <- predict(model, newdata = test_data)\n",
        "                          } else {\n",
        "                            # Compare model_D and model_AD based on AIC (or another metric)\n",
        "                            aic_D <- AIC(model_D)\n",
        "                            aic_AD <- AIC(model_AD)\n",
        "\n",
        "                            if (aic_D < aic_AD) {\n",
        "                              pred <- predict(model_D, newdata = test_data_du)\n",
        "                            } else {\n",
        "                              pred <- predict(model_AD, newdata = test_data_ad)\n",
        "                            }\n",
        "                          }\n",
        "                          predictions[[code]] <- pred\n",
        "                        } else {\n",
        "                          predictions[[code]] <- NA\n",
        "                          cat(\"\\nNot enough test data for ISIC code:\", code, \"after removing NAs\\n\")\n",
        "                        }\n",
        "\n",
        "                      }\n",
        "\n",
        "                      # Check for unchanged CPI row-wise\n",
        "                      if (abs(du_test[2, code] - du_test[1, code]) < 0.04) {\n",
        "                        predictions[[code]] <- 0\n",
        "                      }\n",
        "                    }\n",
        "\n",
        "                    # Ensure all ISIC codes are included in the prediction sheet\n",
        "                    all_codes <- colnames(du)[-1]\n",
        "                    predictions_df <- as.data.frame(matrix(NA, nrow = nrow(du_test_log_diff), ncol = length(all_codes)))\n",
        "                    colnames(predictions_df) <- all_codes\n",
        "                    for (code in all_codes) {\n",
        "                      if (code %in% names(predictions)) {\n",
        "                        predictions_df[[code]] <- predictions[[code]]\n",
        "                      }\n",
        "                    }\n",
        "                    predictions_df <- cbind(date = du_test_log_diff[, 1], predictions_df)\n",
        "                    #file_path=\"C:/Users/SSMEPZ/3D Objects/CP/CPI/CPI Automation/CPI_Input_Files/Estimation_Input_Files/bdd_AJ_.xlsx\"\n",
        "                    # Load the original workbook\n",
        "                    #print(predictions_df)\n",
        "                    wb <- loadWorkbook(temp_pred_path)\n",
        "                    removeWorksheet(wb, \"Predictions\")\n",
        "                    # Write predictions to a new sheet\n",
        "                    addWorksheet(wb, \"Predictions\")\n",
        "                    writeData(wb, \"Predictions\", predictions_df)\n",
        "\n",
        "                    # Save the workbook\n",
        "                    saveWorkbook(wb, temp_pred_path, overwrite = TRUE)\n",
        "\n",
        "                    cat(\"\\nPredictions saved to 'Predictions' sheet in the Excel file.\\n\")\n",
        "                    \"\"\"\n",
        "                    robjects.r(r_code)\n",
        "                    predictions_df= pd.read_excel(Temp_pred_path, sheet_name='Predictions')\n",
        "                    #print('Readed Pred',predictions_df)\n",
        "                    #print(\"\\nPredictions saved to 'Predictions' sheet in the Excel file.\\n\")\n",
        "                    last_row = Emirate.iloc[-1:]\n",
        "                    #print(last_row)\n",
        "                    combined_df = pd.concat([last_row, predictions_df], axis=0, ignore_index=True)\n",
        "                    combined_df = combined_df.fillna(0)\n",
        "                    #print(combined_df)\n",
        "                    isic_Combined = combined_df.columns[1:]\n",
        "                    combined_row = {'date': combined_df['date'].iloc[1]}\n",
        "                    for code in isic_Combined:\n",
        "                        code1='isic0623'\n",
        "                        B1 = combined_df[code].iloc[0]\n",
        "                        B2 = combined_df[code].iloc[1]\n",
        "                        result_value = math.exp(B2) * B1\n",
        "                        combined_row[code] = result_value\n",
        "                    out_df = pd.DataFrame([combined_row])\n",
        "                    #print('Emirates',Emirate)\n",
        "                    #print('OutDF',out_df)\n",
        "                    out_df['date'] = pd.to_datetime(out_df['date']).dt.strftime('%Y-%m-%d')\n",
        "                    Emirate['date'] = pd.to_datetime(Emirate['date']).dt.strftime('%Y-%m-%d')\n",
        "                    Max_date=str(Emirate.iloc[:, 0].max())\n",
        "                    Curr_Date=str(out_df.iloc[:, 0].max())\n",
        "                    #print('Est',Max_date)\n",
        "                    #print('Est',Curr_Date)\n",
        "                    #test = pd.concat([combined_df, out_df], axis=0, ignore_index=True)\n",
        "                    if Max_date!=Curr_Date:\n",
        "                        #with pd.ExcelWriter(file_path, engine='openpyxl', mode='a',if_sheet_exists='replace') as writer:\n",
        "                            #out_df.to_excel(writer, sheet_name='Predictions', index=False)\n",
        "                        #print(\"Predictions saved to 'Predictions' sheet in the Excel file\")\n",
        "                        #Log_DF.loc[len(Log_DF)] = ['Predictions saved to Predictions sheet in the Excel file.', current_date]\n",
        "                        updated_data = pd.concat([Emirate, out_df], ignore_index=True)\n",
        "                        #book = load_workbook(file_path)\n",
        "                        #sheet_name = Sheet_Name\n",
        "                        #sheet = book[sheet_name]\n",
        "                        #last_row = sheet.max_row\n",
        "                        #for col_idx, value in enumerate(out_df.iloc[0], start=1):\n",
        "                            #sheet.cell(row=last_row + 1, column=col_idx, value=value)\n",
        "                        #book.save(file_path)\n",
        "                        #book.close()\n",
        "\n",
        "                        with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
        "                            updated_data.to_excel(writer, index=False, sheet_name=Sheet_Name)\n",
        "                            #print(\"Predictions Updated to\"+Sheet_Name+\"sheet in the Excel file\")\n",
        "                        Log_DF.loc[len(Log_DF)] = ['Predictions Updated to '+Sheet_Name+' sheet in the Excel file.', current_date]\n",
        "                        print('Predictions Updated to '+Sheet_Name+' sheet in the Excel file.')\n",
        "                            #print(updated_data)\n",
        "                        return 1\n",
        "                    else:\n",
        "                        print('Predictions for month '+Curr_Date+' already exist')\n",
        "                        Log_DF.loc[len(Log_DF)] = ['Predictions for month '+Curr_Date+' already exist', current_date]\n",
        "                        print('Predictions for month '+Curr_Date+' already exist')\n",
        "                        return 0\n",
        "                except Exception as e:\n",
        "                    print(str(e))\n",
        "            #==================================Estimations Start ============================================================\n",
        "            Estimation_start_flg=0\n",
        "            Est_UQ_Flg=0\n",
        "            Est_SH_Flg=0\n",
        "            Est_AJ_Flg=0\n",
        "\n",
        "            path=[UMQ_path,AJ_path,SH_path]\n",
        "            sheet=[UMQ_Sheet_Name,AJ_Sheet_Name,SH_Sheet_Name]\n",
        "            #temp=[Temp_pred_path,Temp_pred_path,Temp_pred_path]\n",
        "            #print('Est')\n",
        "            if upd_Abu_data(AJ_path)==1 and upd_dubai_data(AJ_path)==1 and upd_Abu_data(UMQ_path)==1 and upd_dubai_data(UMQ_path)==1 and upd_Abu_data(SH_path)==1 and upd_dubai_data(SH_path)==1:\n",
        "                Estimation_start_flg=1\n",
        "                Log_DF.loc[len(Log_DF)] = ['Abudhabi, Dubai data loaded to Estimation input files', current_date]\n",
        "                print('Abudhabi, Dubai data loaded to Estimation input files')\n",
        "            else:\n",
        "                Estimation_start_flg=0\n",
        "                Log_DF.loc[len(Log_DF)] = ['Abudhabi, Dubai not loaded to Estimations input files', current_date]\n",
        "                print('Abudhabi, Dubai not loaded to Estimations input files')\n",
        "            if Estimation_start_flg==1:\n",
        "                for i,j in zip(path,sheet):\n",
        "                    if j=='UQ':\n",
        "                        if ESTIMATIONS(i,j,Temp_pred_path)==1:\n",
        "                            Est_UQ_Flg=1\n",
        "                            Log_DF.loc[len(Log_DF)] = ['UQ Estimation Completed', current_date]\n",
        "                            print('UQ Estimation Completed')\n",
        "                        else:\n",
        "                            Log_DF.loc[len(Log_DF)] = ['UQ Estimation Not Completed', current_date]\n",
        "                            print('UQ Estimation Not Completed')\n",
        "                    elif j=='AJ':\n",
        "                        if ESTIMATIONS(i,j,Temp_pred_path)==1:\n",
        "                            Est_AJ_Flg=1\n",
        "                            Log_DF.loc[len(Log_DF)] = ['AJ Estimation Completed', current_date]\n",
        "                            print('AJ Estimation Completed')\n",
        "                        else:\n",
        "                            Log_DF.loc[len(Log_DF)] = ['AJ Estimation Not Completed', current_date]\n",
        "                            print('AJ Estimation Not Completed')\n",
        "                    elif j=='SH':\n",
        "                        if ESTIMATIONS(i,j,Temp_pred_path)==1:\n",
        "                            Est_SH_Flg=1\n",
        "                            Log_DF.loc[len(Log_DF)] = ['SH Estimation Completed', current_date]\n",
        "                            print('SH Estimation Completed')\n",
        "                        else:\n",
        "                            Log_DF.loc[len(Log_DF)] = ['SH Estimation Not Completed', current_date]\n",
        "                            print('SH Estimation Not Completed')\n",
        "\n",
        "            else:\n",
        "                print('End the excecution because of AD, DU data not updated to Estimations input files')\n",
        "                Log_DF.loc[len(Log_DF)] = ['End the excecution because of AD, DU data not updated to Estimations input files', current_date]\n",
        "\n",
        "            if Est_UQ_Flg==1 and Est_AJ_Flg==1 and Est_SH_Flg==1:\n",
        "                UQ = pd.DataFrame()\n",
        "                AJ = pd.DataFrame()\n",
        "                SH = pd.DataFrame()\n",
        "\n",
        "                select_col=['Code']\n",
        "\n",
        "                def Cal_Estimations(path,sheet_name):\n",
        "                    Emi=['SH','AJ','UQ']\n",
        "                    if j in Emi:\n",
        "                        read= pd.read_excel(path, dtype=str, engine='openpyxl')\n",
        "                        last_row=read.iloc[-1:]\n",
        "                        Normalized_DF= pd.melt(last_row, id_vars=['date'], var_name='Code', value_name=j+'_value')\n",
        "                    else:\n",
        "                        print(\"Sheet Name Should be emiarte code -  AJ , SH, UQ\")\n",
        "                        Log_DF.loc[len(Log_DF)] = ['Sheet Name Should be emiarte code -  AJ , SH, UQ', current_date]\n",
        "                        #print('Sheet Name Should be emiarte code -  AJ , SH, UQ')\n",
        "                    return Normalized_DF\n",
        "\n",
        "                for i,j in zip(path,sheet):\n",
        "                    if j=='UQ':\n",
        "                        UQ=Cal_Estimations(i,j)\n",
        "                        select_col.append(j+'_value')\n",
        "                    elif j=='AJ':\n",
        "                        AJ=Cal_Estimations(i,j)\n",
        "                        select_col.append(j+'_value')\n",
        "                    elif j=='SH':\n",
        "                        SH=Cal_Estimations(i,j)\n",
        "                        select_col.append(j+'_value')\n",
        "\n",
        "                v_DF1 = pd.merge(UQ,AJ, on='Code', how='inner')\n",
        "                v_DF2 = pd.merge(v_DF1,SH, on='Code', how='inner')\n",
        "                #print(select_col)\n",
        "                Est_Emirates=v_DF2[[col for col in select_col]]\n",
        "                Est_Emirates.loc[:,'Code'] = Est_Emirates['Code'].str.replace('isic', '', regex=False)\n",
        "                #print(Est_Emirates)\n",
        "\n",
        "                #===============RAED Est. Emirates Weightage================================\n",
        "                Est_Emirate_Weight = pd.read_excel(Weight_path, dtype={'Code': str, 'Three_Code': str, 'Two_Code': str}, engine='openpyxl' )\n",
        "                #Est_weight_col=['UQ_LOW','AJ_LOW','SH_LOW','UQ_HIGH','AJ_HIGH','SH_HIGH']\n",
        "                #for i in Est_weight_col:\n",
        "                    #Est_Emirate_Weight[i] = pd.to_numeric(Est_Emirate_Weight[i], errors='coerce')\n",
        "                #print(Est_Emirate_Weight)\n",
        "\n",
        "                #==================================================\n",
        "                Four_Digt_Val= pd.merge(Est_Emirates,Est_Emirate_Weight, on='Code', how='inner')\n",
        "\n",
        "                four_df_col=['UQ_value','AJ_value','SH_value','UQ_LOW','AJ_LOW','SH_LOW','SH_THRE_W','UQ_THRE_W','AJ_THRE_W']\n",
        "\n",
        "                for i in four_df_col:\n",
        "                    Four_Digt_Val[i] = pd.to_numeric(Four_Digt_Val[i], errors='coerce')\n",
        "\n",
        "\n",
        "                #============Alcoholic_Beverages_And_Tobacco_three_codes\n",
        "                NA_Four_digit_codes=['0211']\n",
        "                Four_Digt_Val.loc[Four_Digt_Val['Code'].isin(NA_Four_digit_codes), 'SH_value'] = np.nan\n",
        "\n",
        "                # =====================3 Digit CPI Calculation ===============================================================\n",
        "                #Codes that has single 4 Level\n",
        "                try:\n",
        "                    mask_req_thr_codes = Four_Digt_Val['Three_Code'].isin(['021','023','032','041','042','052','054','063','064','082','091','094','098','101','102','104','105','112','122','133','139'])\n",
        "\n",
        "                    Four_Digt_Val.loc[mask_req_thr_codes,'WEIGHT_MUL_UQ'] = (Four_Digt_Val['UQ_value'] * Four_Digt_Val['UQ_THRE_W'])/100\n",
        "                    Four_Digt_Val.loc[mask_req_thr_codes,'WEIGHT_MUL_AJ'] = (Four_Digt_Val['AJ_value'] * Four_Digt_Val['AJ_THRE_W'])/100\n",
        "                    Four_Digt_Val.loc[mask_req_thr_codes,'WEIGHT_MUL_SH'] = (Four_Digt_Val['SH_value'] * Four_Digt_Val['SH_THRE_W'])/100\n",
        "\n",
        "                    Four_Digt_Val.loc[~mask_req_thr_codes,'WEIGHT_MUL_UQ'] = (Four_Digt_Val['UQ_value'] * Four_Digt_Val['UQ_LOW'])/100\n",
        "                    Four_Digt_Val.loc[~mask_req_thr_codes,'WEIGHT_MUL_AJ'] = (Four_Digt_Val['AJ_value'] * Four_Digt_Val['AJ_LOW'])/100\n",
        "                    Four_Digt_Val.loc[~mask_req_thr_codes,'WEIGHT_MUL_SH'] = (Four_Digt_Val['SH_value'] * Four_Digt_Val['SH_LOW'])/100\n",
        "                except Exception as e:\n",
        "                    print(str(e))\n",
        "\n",
        "                v_three_UQ = Four_Digt_Val.groupby('Three_Code', as_index=False)['WEIGHT_MUL_UQ'].sum()\n",
        "                v_three_UQ.columns=['Code','WEIGHT_MUL_UQ']\n",
        "\n",
        "                v_three_AJ = Four_Digt_Val.groupby('Three_Code', as_index=False)['WEIGHT_MUL_AJ'].sum()\n",
        "                v_three_AJ.columns=['Code','WEIGHT_MUL_AJ']\n",
        "\n",
        "                v_three_SH = Four_Digt_Val.groupby('Three_Code', as_index=False)['WEIGHT_MUL_SH'].sum()\n",
        "                v_three_SH.columns=['Code','WEIGHT_MUL_SH']\n",
        "\n",
        "                v_three_df= pd.merge(v_three_UQ,v_three_AJ, on='Code', how='inner')\n",
        "                v1_three_df= pd.merge(v_three_df,v_three_SH, on='Code', how='inner')\n",
        "\n",
        "\n",
        "\n",
        "                v2_three_df= pd.merge(v1_three_df,Est_Emirate_Weight, on='Code', how='inner')\n",
        "                #print(v2_three_df)\n",
        "                #print('v2_three_df',v2_three_df)\n",
        "\n",
        "                v2_three_df['CPI_UQ']=(v2_three_df['WEIGHT_MUL_UQ'] / v2_three_df['UQ_LOW'])*100\n",
        "                v2_three_df['CPI_AJ']=(v2_three_df['WEIGHT_MUL_AJ'] / v2_three_df['AJ_LOW'])*100\n",
        "                v2_three_df['CPI_SH']=(v2_three_df['WEIGHT_MUL_SH'] / v2_three_df['SH_LOW'])*100\n",
        "\n",
        "                Three_code_col=['Code','Two_Code','CPI_UQ','CPI_AJ','CPI_SH']\n",
        "                three_dig_CPI=v2_three_df[Three_code_col]\n",
        "\n",
        "                #print(three_dig_CPI)\n",
        "\n",
        "                #============Alcoholic_Beverages_And_Tobacco_three_codes\n",
        "                #NA_Three_digit_codes=['021','023']\n",
        "                #three_dig_CPI.loc[three_dig_CPI['Code'].isin(NA_Three_digit_codes), 'CPI_SH'] = np.nan\n",
        "\n",
        "                #print(three_dig_CPI)\n",
        "\n",
        "                #=================================3 Digit CPI calc END ====================================================\n",
        "\n",
        "                #==================================2 digit Calc Start =========================================\n",
        "\n",
        "                v_two_Digt_Val= pd.merge(three_dig_CPI[['Code','CPI_UQ','CPI_AJ','CPI_SH']],Est_Emirate_Weight, on='Code', how='inner')\n",
        "                #print(v_two_Digt_Val)\n",
        "\n",
        "                v_two_Digt_Val['WEIGHT_MUL_UQ'] = (v_two_Digt_Val['CPI_UQ'] * v_two_Digt_Val['UQ_LOW'])/100\n",
        "                v_two_Digt_Val['WEIGHT_MUL_AJ'] = (v_two_Digt_Val['CPI_AJ'] * v_two_Digt_Val['AJ_LOW'])/100\n",
        "                v_two_Digt_Val['WEIGHT_MUL_SH'] = (v_two_Digt_Val['CPI_SH'] * v_two_Digt_Val['SH_LOW'])/100\n",
        "\n",
        "                #print(v_two_Digt_Val)\n",
        "                v_two_UQ = v_two_Digt_Val.groupby('Two_Code', as_index=False)['WEIGHT_MUL_UQ'].sum()\n",
        "                v_two_UQ.columns=['Code','WEIGHT_MUL_UQ']\n",
        "                #print(v_two_UQ)\n",
        "                v_two_AJ = v_two_Digt_Val.groupby('Two_Code', as_index=False)['WEIGHT_MUL_AJ'].sum()\n",
        "                v_two_AJ.columns=['Code','WEIGHT_MUL_AJ']\n",
        "\n",
        "                v_two_SH = v_two_Digt_Val.groupby('Two_Code', as_index=False)['WEIGHT_MUL_SH'].sum()\n",
        "                v_two_SH.columns=['Code','WEIGHT_MUL_SH']\n",
        "\n",
        "                v_two_df= pd.merge(v_two_UQ,v_two_AJ, on='Code', how='inner')\n",
        "                v1_two_df= pd.merge(v_two_df,v_two_SH, on='Code', how='inner')\n",
        "\n",
        "                #print(v1_two_df)\n",
        "                v2_two_df= pd.merge(v1_two_df,Est_Emirate_Weight, on='Code', how='inner')\n",
        "\n",
        "                #print(v2_two_df)\n",
        "                v2_two_df[prc_month+'_'+prc_year+'_UQ']=(v2_two_df['WEIGHT_MUL_UQ'] / v2_two_df['UQ_LOW'])*100\n",
        "                v2_two_df[prc_month+'_'+prc_year+'_AJ']=(v2_two_df['WEIGHT_MUL_AJ'] / v2_two_df['AJ_LOW'])*100\n",
        "                v2_two_df[prc_month+'_'+prc_year+'_SH']=(v2_two_df['WEIGHT_MUL_SH'] / v2_two_df['SH_LOW'])*100\n",
        "\n",
        "                #print(v2_two_df)\n",
        "\n",
        "                two_code_col=['Code',prc_month+'_'+prc_year+'_UQ',prc_month+'_'+prc_year+'_AJ',prc_month+'_'+prc_year+'_SH']\n",
        "                two_dig_CPI=v2_two_df[two_code_col]\n",
        "\n",
        "                #print(two_dig_CPI)\n",
        "\n",
        "                #===================Two Digit CPI Calc END =================================\n",
        "\n",
        "                #===================== '00' Level CPI calc Start ===========================\n",
        "\n",
        "                #print(two_dig_CPI)\n",
        "                v_zero_Digt_Val= pd.merge(two_dig_CPI[['Code',prc_month+'_'+prc_year+'_UQ',prc_month+'_'+prc_year+'_AJ',prc_month+'_'+prc_year+'_SH']],Est_Emirate_Weight, on='Code', how='inner')\n",
        "\n",
        "\n",
        "                v_zero_Digt_Val['WEIGHT_MUL_UQ'] = (v_zero_Digt_Val[prc_month+'_'+prc_year+'_UQ'] * v_zero_Digt_Val['UQ_LOW'])/100\n",
        "                v_zero_Digt_Val['WEIGHT_MUL_AJ'] = (v_zero_Digt_Val[prc_month+'_'+prc_year+'_AJ'] * v_zero_Digt_Val['AJ_LOW'])/100\n",
        "                v_zero_Digt_Val['WEIGHT_MUL_SH'] = (v_zero_Digt_Val[prc_month+'_'+prc_year+'_SH'] * v_zero_Digt_Val['SH_LOW'])/100\n",
        "\n",
        "                #print(v_zero_Digt_Val)\n",
        "\n",
        "                v_total_UQ = v_zero_Digt_Val['WEIGHT_MUL_UQ'].sum()\n",
        "                #total_UQ=(v_total_UQ/0.009065)*100\n",
        "                total_UQ=(v_total_UQ/1.0)*100\n",
        "                v_zero_UQ = pd.DataFrame({'Code': ['00'], prc_month+'_'+prc_year+'_UQ': [total_UQ]})\n",
        "\n",
        "                v_total_AJ = v_zero_Digt_Val['WEIGHT_MUL_AJ'].sum()\n",
        "                #total_AJ=(v_total_AJ/0.055441)*100\n",
        "                total_AJ=(v_total_AJ/1.0)*100\n",
        "                v_zero_AJ = pd.DataFrame({'Code': ['00'], prc_month+'_'+prc_year+'_AJ': [total_AJ]})\n",
        "\n",
        "                v_total_SH = v_zero_Digt_Val['WEIGHT_MUL_SH'].sum()\n",
        "                #total_SH=(v_total_SH/0.180239)*100\n",
        "                total_SH=(v_total_SH/1.0)*100\n",
        "\n",
        "                v_zero_SH = pd.DataFrame({'Code': ['00'], prc_month+'_'+prc_year+'_SH': [total_SH]})\n",
        "\n",
        "                v_zero_df= pd.merge(v_zero_UQ,v_zero_AJ, on='Code', how='inner')\n",
        "                v1_zero_df= pd.merge(v_zero_df,v_zero_SH, on='Code', how='inner')\n",
        "\n",
        "                #print(v1_zero_df)\n",
        "\n",
        "                #=================================Zero Level CPI Calc END =============================\n",
        "\n",
        "                #======================= PRINT ALL LEVELS CPI =========================================\n",
        "\n",
        "                out_four_code_cpi=Four_Digt_Val[['Code','UQ_value','AJ_value','SH_value']]\n",
        "                out_four_code_cpi.columns=['Code',prc_month+'_'+prc_year+'_UQ',prc_month+'_'+prc_year+'_AJ',prc_month+'_'+prc_year+'_SH']\n",
        "                #print(out_four_code_cpi)\n",
        "                out_three_code_cpi=three_dig_CPI[['Code','CPI_UQ','CPI_AJ','CPI_SH']]\n",
        "                out_three_code_cpi.columns=['Code',prc_month+'_'+prc_year+'_UQ',prc_month+'_'+prc_year+'_AJ',prc_month+'_'+prc_year+'_SH']\n",
        "                v_UQ_AJ_SH_CPI_VAL= pd.concat([v1_zero_df, two_dig_CPI, out_three_code_cpi,out_four_code_cpi], ignore_index=True)\n",
        "                estimation_flg=1\n",
        "                #print(estimation_flg,'EST')\n",
        "                return v_UQ_AJ_SH_CPI_VAL\n",
        "            else:\n",
        "                print('Error while AJ, SH, UQ predictions estimation')\n",
        "                Log_DF.loc[len(Log_DF)] = ['Error while AJ, SH, UQ predictions estimation', current_date]\n",
        "                #sys.exist()\n",
        "        except Exception as e:\n",
        "            Log_DF.loc[len(Log_DF)] = ['Estimation Phase '+str(e), current_date]\n",
        "    else:\n",
        "        print(\"Not Enough Emirates readed for CPI estimation\")\n",
        "        Log_DF.loc[len(Log_DF)] = ['Not Enough Emirates readed for CPI estimation', current_date]\n",
        "#=======================================================================================\n",
        "\n",
        "def Calc_CPI(DF,E_Weightage):\n",
        "    print('CPI Calculation Started..')\n",
        "    try:\n",
        "        Readed_col=[prc_month+'_'+prc_year+'_FUJ'\n",
        "        ,prc_month+'_'+prc_year+'_AJ'\n",
        "        ,prc_month+'_'+prc_year+'_RAK'\n",
        "        ,prc_month+'_'+prc_year+'_UQ'\n",
        "        ,prc_month+'_'+prc_year+'_ABU'\n",
        "        ,prc_month+'_'+prc_year+'_DUB'\n",
        "        ,prc_month+'_'+prc_year+'_SH']\n",
        "        for column in Readed_col:\n",
        "            DF[column] = pd.to_numeric(DF[column], errors='coerce')\n",
        "\n",
        "        #print(DF)\n",
        "\n",
        "        v_Sum_of_CPI=pd.merge(DF,E_Weightage, on='CODE', how='inner')\n",
        "        #print(v_Sum_of_CPI)\n",
        "        v_Sum_of_CPI['WEIGHTED_SUM'] = (\n",
        "            (v_Sum_of_CPI[prc_month+'_'+prc_year+'_FUJ'].fillna(0) * v_Sum_of_CPI['FUJ_WEIGHT'])/100 +\n",
        "            (v_Sum_of_CPI[prc_month+'_'+prc_year+'_AJ'].fillna(0) * v_Sum_of_CPI['AJM_WEIGHT'])/100 +\n",
        "            (v_Sum_of_CPI[prc_month+'_'+prc_year+'_RAK'].fillna(0) * v_Sum_of_CPI['RAK_WEIGHT'])/100 +\n",
        "            (v_Sum_of_CPI[prc_month+'_'+prc_year+'_UQ'].fillna(0) * v_Sum_of_CPI['UQM_WEIGHT'])/100 +\n",
        "        \t(v_Sum_of_CPI[prc_month+'_'+prc_year+'_ABU'].fillna(0) * v_Sum_of_CPI['ABU_WEIGHT'])/100 +\n",
        "        \t(v_Sum_of_CPI[prc_month+'_'+prc_year+'_DUB'].fillna(0) * v_Sum_of_CPI['DUB_WEIGHT'])/100 +\n",
        "        \t(v_Sum_of_CPI[prc_month+'_'+prc_year+'_SH'].fillna(0) * v_Sum_of_CPI['SHJ_WEIGHT'])/100\n",
        "        )\n",
        "\n",
        "        #print(v_Sum_of_CPI)\n",
        "\n",
        "        #v_Sum_of_CPI[prc_month+'_'+prc_year+'_UAE']=(v_Sum_of_CPI['WEIGHTED_SUM']/(v_Sum_of_CPI['STATE_WEIGHT_BY_STATE_TO_HOUSEHOLDS'])*100)\n",
        "        #MUL_EMI_WEIGHTS\n",
        "        v_Sum_of_CPI[prc_month+'_'+prc_year+'_UAE']=(v_Sum_of_CPI['WEIGHTED_SUM']/(v_Sum_of_CPI['ALL_EMI_MUL_WEIGHT'])*100)\n",
        "        #print(v_Sum_of_CPI)\n",
        "        #Out_CPI=Sum_of_CPI[['CODE','UAE']]\n",
        "        Out_DF=v_Sum_of_CPI[['CODE',prc_month+'_'+prc_year+'_FUJ',prc_month+'_'+prc_year+'_AJ',prc_month+'_'+prc_year+'_RAK',prc_month+'_'+prc_year+'_UQ',prc_month+'_'+prc_year+'_ABU',prc_month+'_'+prc_year+'_DUB',prc_month+'_'+prc_year+'_SH',prc_month+'_'+prc_year+'_UAE']]\n",
        "        columns_to_check = [col for col in Out_DF.columns if col != prc_month+'_'+prc_year+'_UAE' and 'CODE']\n",
        "        def update_total(row):\n",
        "            if row[columns_to_check].isna().all():\n",
        "                return np.nan\n",
        "            return row[prc_month+'_'+prc_year+'_UAE']\n",
        "        Out_DF.loc[:,prc_month+'_'+prc_year+'_UAE'] = Out_DF.apply(update_total, axis=1)\n",
        "        #print(Out_DF)\n",
        "        zero_lvl_weight=pd.merge(Out_DF,E_Weightage, on='CODE', how='inner')\n",
        "        v2_zero_Lvl_Cal=zero_lvl_weight[zero_lvl_weight['CODE'].isin(['01','02','03','04','05','06','07','08','09','10','11','12','13'])][['CODE',prc_month+'_'+prc_year+'_UAE','GLOBALE']]\n",
        "        v2_zero_Lvl_Cal['MUL_EMI_VALUE'] = ((v2_zero_Lvl_Cal[prc_month+'_'+prc_year+'_UAE'].fillna(0) * v2_zero_Lvl_Cal['GLOBALE']))\n",
        "        sum_of_emi_val=v2_zero_Lvl_Cal['MUL_EMI_VALUE'].sum()\n",
        "        gloabl_weight_zero_lvl=v_Sum_of_CPI.loc[v_Sum_of_CPI['CODE'] =='00', 'GLOBALE'].loc[0]\n",
        "        zero_CODE_val=sum_of_emi_val/gloabl_weight_zero_lvl\n",
        "        Out_DF.loc[Out_DF['CODE'] == '00', prc_month+'_'+prc_year+'_UAE'] = zero_CODE_val\n",
        "        print(Out_DF)\n",
        "        print('CPI Calculations Completed................')\n",
        "        Log_DF.loc[len(Log_DF)] = ['CPI Calculations Completed', current_date]\n",
        "        return Out_DF\n",
        "    except Exception as e:\n",
        "        print('CPI calculation Failed : '+str(e))\n",
        "        Log_DF.loc[len(Log_DF)] = ['CPI calculation Failed '+str(e), current_date]\n",
        "\n",
        "\n",
        "Out_EMIRATE_CPI=pd.DataFrame()\n",
        "UQ_AJ_SH_CPI_VAL=pd.DataFrame()\n",
        "if coicop_flg==1 and emi_weight_flg==1 and fuj_rd_flg==1 and rak_rd_flg==1 and abd_rd_flg==1 and dub_rd_flg==1:\n",
        "    UQ_AJ_SH_CPI_VAL=Estimation_Phase(1)\n",
        "    #print(estimation_flg,'EST')\n",
        "    #print(UQ_AJ_SH_CPI_VAL)\n",
        "    if estimation_flg==1:\n",
        "        UQ_AJ_SH_CPI_VAL.rename(columns={'Code': 'CODE'}, inplace=True)\n",
        "        Con_All_Emi_CPI = pd.merge(Consoldtd_DF, UQ_AJ_SH_CPI_VAL, on='CODE', how='outer')\n",
        "        print(Con_All_Emi_CPI)\n",
        "        Out_EMIRATE_CPI=Calc_CPI(Con_All_Emi_CPI,Emirate_Weightage)\n",
        "        arch_flg=1\n",
        "else:\n",
        "    print('Error While reading input files')\n",
        "    Log_DF.loc[len(Log_DF)] = ['Error While reading input files', current_date]\n",
        "\n",
        "#print(Out_EMIRATE_CPI)\n",
        "\n",
        "def ARCH_PRC(arch_flag):\n",
        "    if arch_flag==1:\n",
        "        print('Started wrting to ARCH files')\n",
        "        try:\n",
        "            ARCH_path=Output_Path\n",
        "            start_row=1\n",
        "            end_row=170\n",
        "            ARCH_DF = pd.read_excel(ARCH_path, dtype={'CODE':str}, engine='openpyxl' )\n",
        "            #print(ARCH_DF)\n",
        "            #print(Out_EMIRATE_CPI)\n",
        "            updated_df = pd.merge(ARCH_DF, Out_EMIRATE_CPI, on='CODE', how='left')\n",
        "            #print(updated_df)\n",
        "            try:\n",
        "                arch_col_to_chk=[prc_month+'_'+prc_year+'_UAE']\n",
        "                for arch_col in arch_col_to_chk:\n",
        "                    if arch_col not in ARCH_DF.columns:\n",
        "                        with pd.ExcelWriter(Output_Path, engine='openpyxl') as writer:\n",
        "                            updated_df.to_excel(writer, sheet_name='Sheet1', index=False)\n",
        "                        print(\"ARCH Data updated successfully\")\n",
        "                        Log_DF.loc[len(Log_DF)] = ['ARCH Data updated successfully', current_date]\n",
        "                    else:\n",
        "                        print(f\"The processing month: {arch_col} already exists in the ARCH output file.\")\n",
        "                        Log_DF.loc[len(Log_DF)] = ['The processing month already exists in the ARCH output file.', current_date]\n",
        "            except Exception as e:\n",
        "                print('An error occurred while writing the DataFrame to Excel:'+str(e))\n",
        "                Log_DF.loc[len(Log_DF)] = ['An error occurred while writing the DataFrame to Excel:'+str(e), current_date]\n",
        "        except Exception as e:\n",
        "            print('An error occurred while writing into ARCH process:'+str(e))\n",
        "            Log_DF.loc[len(Log_DF)] = ['An error occurred while writing into ARCH process: '+str(e), current_date]\n",
        "\n",
        "ARCH_PRC(arch_flg)\n",
        "\n",
        "os.makedirs(os.path.dirname(Log_path), exist_ok=True)\n",
        "Log_DF.to_csv(Log_path, sep='\\t', index=False)\n",
        "\n",
        "\n",
        "source_path = Output_Path\n",
        "destination_path = '/content/sample_data/Copy_Of_ARCH_TimeSeries.xlsx'\n",
        "\n",
        "if os.path.exists(destination_path):\n",
        "    os.remove(destination_path)\n",
        "\n",
        "shutil.copy(source_path, destination_path)\n",
        "\n",
        "print(f\"File copied to {destination_path}\")\n",
        "print('----------------------------------------------------------------------------------------------------------------------')\n",
        "\n",
        "\n"
      ]
    }
  ]
}